---
title: "My First Steps with Pandas"
datePublished: Thu Mar 13 2025 19:46:09 GMT+0000 (Coordinated Universal Time)
cuid: cm87rer2z000009ks859w5mdj
slug: my-first-steps-with-pandas
cover: https://cdn.hashnode.com/res/hashnode/image/upload/v1741894949389/69ddbebd-d3a1-4f63-9b73-cbac7a30bb14.jpeg
tags: machine-learning, pandas, numpy

---

Welcome back! In my last blog, I shared some cool things I learned about NumPy. Today, I’m excited to talk about some important operations in Pandas!

You might be wondering, why should we bother with Pandas? Well, in the world of machine learning, data really is king! We often work with large datasets—like Excel spreadsheets or CSV files—that can have issues like missing values, duplicates, or they just need a bit of cleaning up. This is where Pandas shines! It helps us clean the data, handle those missing values, filter out what we don’t need, group and aggregate information, and even merge different datasets together.

Think about it: trying to build a machine learning model with messy data is like trying to drive a super fast car while wearing a blindfold—it’s just not going to work! So having clean data is crucial.

Alright, enough chit-chat! Let’s dive into some handy Pandas operations that we’ll be using in our future projects. Let’s get started!

### ***Series -***

It is like a single column of data with labels(index). It is like a to-do list where each task(data point) has a priority number (index).

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1741883113204/737379a1-2000-464b-9696-c8562217d171.png align="center")

### ***Data Frame -***

It is like a full Excel table with rows and columns.

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1741885793061/b5300000-0d16-4f6f-9cec-605aa2c649a5.png align="center")

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1741885803544/0afbad28-67e0-4e5c-80fa-9757cb07749c.png align="center")

### ***Loading and Exploring the Dataset -***

There are many ways to load a dataset :

```python
# Loading a CSV file from a local file: 
df = pd.read_csv('dataset.csv')

#loading from a URL 
url ='go_to_github_click_raw_copy_the_url_'
df = pd.read_csv(url) # for csv files

#loading an excel file 
df = pd.read_excel('data.xlsx')
```

I am going to use the following Titanic Dataset:

[https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv](https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv)

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1741887756042/8d222cb3-b8d1-446b-bd2e-0c909c4f5c92.png align="center")

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1741887765282/f27352e1-713a-4f91-a420-7017a2ec7ecc.png align="center")

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1741887783162/16cbd50c-8b32-4c5c-9f4a-1db84aac7897.png align="center")

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1741887793576/a3781749-dc37-41aa-bf99-bc31579a09ae.png align="center")

### ***Pivot Table :***

Pivot table is like the “boss lady“ of data summarization - super powerful and elegant! It is used to summarize, aggregate and analyze data in a flexible way.

**General syntax :** `pd.pivot_table(dataframe,values,index,columns,aggfun)`

*values \- columns you want to calculate*

*index - rows to group by*

*columns - columns to group by*

*aggfunc - what functions to apply (like mean ,sum,etc.)*

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1741888338261/26fdf597-b1b8-46e0-b900-3f68307b3f44.png align="center")

The above pivot table groups the data by **Passenger Class (1st, 2nd, 3rd class)** as rows and **Gender (Male/Female)** as columns, then calculates the **average survival rate** from the **'survived' column** using the **mean function**.

### ***Handling Missing Values in the Titanic Dataset:***

Handling missing values in the Titanic dataset is super important because, you know, when data is incomplete, it can mess up the whole analysis. Imagine trying to guess someone's age without knowing  
their birth year — kinda confusing, right?

So, in this dataset, columns like 'Age', 'Cabin', and 'Embarked' have missing values, and if we just ignore them, it'll lead to incorrect results and poor model performance. We need to handle these gaps smartly to keep the data accurate and meaningful.

There are a few ways to deal with this. We can either remove the rows or columns with too many missing values or fill them with something meaningful, like the average age or the most common value. Sometimes, we can even use machine learning models to predict the missing data.

Let’s dive into the actual process of handling these missing values step by step :

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1741889523607/3a557813-9d3c-4b28-83b1-84428f50bf26.png align="center")

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1741889513812/ce24048d-26af-4678-b101-3f64f574b4f3.png align="center")

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1741889648599/0fc1d155-0e16-4dc3-95b4-89e40e792af9.png align="center")

### ***Data Filtering and Selection :***

Filtering and selecting data is like picking out your favorite outfits from a massive wardrobe — you only want what fits your vibe, right?

In this dataset, we often need to work with specific rows or columns based on certain conditions. For instance, filtering out only female passengers or selecting data for those who survived. If we don’t do this properly, we’ll end up with irrelevant information that could mess up our analysis.

There are multiple ways to do this. We can use **conditional filtering**, where we select rows based on conditions like age or class. Or we can go for **column selection**, where we pick only the columns that matter to us, like 'Survived' or 'Fare'.

Let’s dive in and explore these methods step by step :

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1741890696890/90e0caa6-1d96-4b79-869b-cec08fa5dbf3.png align="center")

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1741890709398/49d99c93-8dd6-4450-ab64-2a92d4d90ce9.png align="center")

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1741890723937/95fd528a-d43a-415f-a4c8-e4e6acc3c46e.png align="center")

### ***Grouping and Aggregation :***

Grouping and aggregation in data analysis is like organizing your playlist by artist or genre — it helps you spot trends and patterns effortlessly.

Grouping involves splitting data into categories based on shared characteristics.

Aggregation comes in when you want to perform calculations on these groups, like finding the average sales, total revenue, or the number of products sold.

Let’s dive in and explore how grouping and aggregation can make data analysis more powerful.

`groupby()` - Groups data based on one or more columns.

**General syntax -** `df.groupby('column_name')['target_column'].operation()`

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1741892303137/4675e87e-298b-476d-abc9-22893af0a04c.png align="center")

`agg()`\-Performs multiple aggregation functions at once.

**General Syntax -** `df.agg({'column1': 'func1', 'column2': 'func2'})`

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1741892355340/2e0293d7-1267-4699-8843-f46a58799163.png align="center")

`sort_values()`\-Sorts the DataFrame based on values in a specific column.

**General Syntax -** `df.sort_values('column_name', ascending=True/False)`

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1741892215774/cf5ae72a-9878-4208-9745-84c7e4c337cc.png align="center")

`cumsum()`\- Calculates the cumulative sum.

**General Syntax -** `df['column_name'].cumsum()`

![](https://cdn.hashnode.com/res/hashnode/image/upload/v1741892176881/cfabf6d5-e4a1-45b2-8bd0-e3c39408b6d2.png align="center")

And that’s it! This blog was just me sharing what I’ve learned about Pandas so far, and I hope it helped you in some way. I’m definitely not a pro, just someone exploring and trying to understand things better.

If you have any feedback or suggestions, or if I got something wrong, I’d love to hear from you! Learning is always more fun when it’s a two-way conversation. So feel free to share your thoughts!

Let’s keep learning together.